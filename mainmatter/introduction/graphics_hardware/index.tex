\section{Graphics Hardware}\label{sec:graphics_hardware}


\begin{sectionmeta}
	
	This section will introduce the \gls{GPU} from a hardware standpoint. 
	First the overall concept of a \gls{GPU} will be described - what it is, what it does, and how it achieve its purpose.
	
	\cite{intro_to_gpu_arch} describes the architecture and components of a \gls{GPU} as the result of three ideas.
	These ideas will be presented here.
	Different terminology for the individual components will be presented as they are used by the \gls{GPU} vendors, NVidia and AMD. 
	
\end{sectionmeta}


\subsection{GPU as a Concept}
The \gls{GPU} has been developed with a specific domain in mind, as opposed to the CPU, which is for general purposes. 
The domain of the \gls{GPU} was originally only image manipulation -- a field, where a \gls{SIMD} architecture has proven useful \todo{citation needed}.
%In recent years, however, there has been a fo\glss on using the \gls{GPU} for a broader spectrum of applications, the socalled \gls{GPGPU}. 

\fig{figures/graphics_hdw_cpu_style_core}{\gls{CPU} style core \cite[p. 14]{intro_to_gpu_arch}}{cpuStyleCore}{1}

Figure \ref{fig:cpuStyleCore} shows a visual representation of a \gls{CPU} style core. 
The red boxes "Out-of-order control logic", "Fancy branch predictor" and "Memory pre-fetcher" all have to do with predicting/preventing stalls in the \gls{CPU}.
These features are not too important for the \gls{GPU}, since its main focus is throughput \todo{citation needed}. 
Furthermore, a big cache would limit the amount of cores a single chip could hold, so this is not desirable for a \gls{GPU} either.
The remaining components -- Fetch/Decode, \gls{ALU} and Execution Context -- is described below.

\paragraph{The Fetch/Decode Component} handles retrieving data from memory and storing it in the Execution Context.

\paragraph{The \gls{ALU}} performs the actual computations on the fetched data. Any temporary variables or conditions (when handling branches) are stored or retrieved from the Execution Context.

\paragraph{The Execution Context} contains local data, e.g. variables and conditions, needed to perform the current computation.

\subsection{\acs{CPU} Style Core}
As previously described, there are components in the \gls{CPU} style core which are not needed for the \gls{GPU} to achieve a high throughput.
So the first idea presented by \citet{intro_to_gpu_arch} is to "slim down" the core by getting rid of these components.
The result of slimming down the core can be seen in Figure \ref{fig:twoSlimCores}.

\fig{figures/graphics_hdw_two_cores}{Two slimmed down cores \cite[p. 16]{intro_to_gpu_arch}}{twoSlimCores}{1}

\fig{figures/graphics_hdw_shader_code}{A closer look at the shader code in \ref{fig:twoSlimCores} - \citet[p. 22]{intro_to_gpu_arch}}{shaderCode}{0.5}

Figure \ref{fig:twoSlimCores} presents two slimmed down cores running two fragments in parallel. 
Each core runs the same code, but since the contents of the Execution Contexts are different, we achieve the desired \gls{SIMD} effect; the two fragments are processed by the same code (Single Instruction), but the code refers to data through registers in the Execution Context, which is different for the two cores (Multiple Data).

Figure \ref{fig:shaderCode} displays the shader code run on fragment 1 and 2 from \ref{fig:twoSlimCores}.

\subsection{Slimmed Down Core}
Fetching data and instructions are a relatively time-costly activity for the core \todo{citation needed}.
The second idea, to further the throughput of the \gls{GPU}, is to let multiple \glspl{ALU} share a single Fetch component.
This way, the component need to retrieve more information at a time, less times, which is not as costly as retrieving small bits of information (data and instructions) for a single \gls{ALU}.
This means the actual instruction which needs to be run on each \gls{ALU} can be fetched only once per core instead of once per \gls{ALU}.

\fig{figures/graphics_hdw_shared_fetch}{Fetch component shared by eight \glspl{ALU}. Note the instructions need to change to use vector operations on vector data as well \cite[p. 24]{intro_to_gpu_arch}}{sharedFetch}{1}

Figure \ref{fig:sharedFetch} shows an example of a core with eight \glspl{ALU} sharing a single instruction stream (i.e. Fetch component).
Since the instructions need to be carried out on a vector of data (each element in the vector corresponding to data for a single \gls{ALU}).
The instructions need to reflect this change from single data to vector of data -- hence the change from "mul" and "madd" from \ref{fig:twoSlimCores} to "VEC8\_mul" and "VEC8\_madd" in Figure \ref{fig:sharedFetch} in the shader.

Since the core now contains multiple contexts, the Execution Context component will now be referred to as the \textbf{Shared Context component}.

\subsection{Hiding Stalling}
The final thing \glspl{GPU} do to achieve a high throughput is to hide stalling by storing multiple contexts for different fragments on a single core. 
This allows the core to switch which fragment it works on once the current fragment stalls.
Stalling occurs when the processing of a fragment group is dependent on another fragment group which is not done processing itself (recall the \glspl{ALU} each work on its own fragment, as per the slimmed down cores.  The fragments being worked on by the \glspl{ALU} at the same time constitutes a fragment group).
Because switching between contexts is faster than waiting for the context to come out of a stall, the amount of time the \gls{GPU} takes to perform a job is lessened by the hiding of stalls; the latency of the \gls{GPU} caused by stalling has been hidden, which is why this process is also known as latency hiding.

This latency hiding through interleaving execution of groups of fragments is done because the first idea \todo{ref?} stripped the core of the means the \gls{CPU} uses to hide stalling.

\fig{figures/graphics_hdw_hiding_stalls_1}{The shared context data is split up to match the (here four) different fragment groups \cite[p. 35]{intro_to_gpu_arch}}{hidingStalls1}{1}

\fig{figures/graphics_hdw_hiding_stalls_2}{When one fragment stalls, the \gls{GPU} switches to another stored context and continues on another fragment \cite[p. 37]{intro_to_gpu_arch}}{hidingStalls2}{1}

The figures \ref{fig:hidingStalls1} and \ref{fig:hidingStalls2} shows the \gls{GPU} latency hiding process.
First the Shared Context component is divided into the number of fragment groups the core shall be able to process -- four in this example.
Then the processing of the first fragment group is begun. 
Once it stalls (or completes), the processing of the next fragment group can begin.
The idea is: Once all fragment groups have been cycled trough, the cause of the stall of the first fragment group has been resolved.

\subsection{Branching in \glspl{GPU}}
One last thing worth noting from \cite{intro_to_gpu_arch} is how \glspl{GPU} handle branching.

\fig{figures/graphics_hdw_branching}{How \glspl{GPU} handle branching \cite[p. 29]{intro_to_gpu_arch}}{branching}{1}

Figure \ref{fig:branching} shows how branching is handled within a single core: The instructions for both branches are executed (on different \glspl{ALU}), and later the correct result will be chosen.

\subsection{Contemporary Graphics Cards}
This subsection will describe the architecture of  contemporary NVIDIA, Intel and AMD graphics cards and compare the terminology used by these vendors to the terminology presented in this section.
The selected graphics cards are:
\begin{itemize}
	\item NVIDIA GeForce GTX 1060
	\item Intel HD Graphics 4600
	\item AMD Sapphire Radeon R9 280 3GB GDDR5
\end{itemize}

\subsubsection{NVIDIA}
The NVIDIA GeForce GTX 1060 graphics card is build with the NVIDIA Pascal architecture \cite{nvidia_gtx_1060}.

\fig{figures/graphics_hdw_pascal_sm}{A Streaming Multiprocessor in the Pascal architecture. - \cite{nvidia_tesla_p100} p. 13}{pascalSM}{1}

Figure \ref{fig:pascalSM} shows a \gls{SM} in the Pascal architecture.
To translate the terminology from NVIDIA to what has been presented in this section, the small rectangles labelled "Core", "DP Unit", "LD/ST" and "SFU" all fall under the category \gls{ALU} as presented earlier (technically, each of these components consists of multiple \glspl{ALU}).
These are in NVIDIA terminology collectively referred to as "CUDA Cores".

The blue caches, textures and buffers are all part of the Shared Context component.

The Fetch/Decode component is not visible in figure \ref{fig:pascalSM}.

From the above translation of terminology, the \gls{SM} seems to map quite well onto what has been presented as the "core".
However, it is obvious that there exist a further grouping inside the \gls{SM}: The left-hand side and the right-hand side.
Even though the two sides share an instruction stream and some memory, they are still two distinct sides.
These sides are what is referred to as "Warps" in NVIDIA terminology.

\fig{figures/graphics_hdw_pascal_gpu}{A 60 \gls{SM} units Pascal GP100 GPU - \cite{nvidia_tesla_p100} p. 10}{pascalGPU}{0.8}

Figure \ref{fig:pascalGPU} shows a complete GP100 GPU in the Pascal Architecture.
Two \glspl{SM} are grouped into one \glspl{TPC}, and five \glspl{TPC} are grouped together into a \gls{GPC}. \\

It should be noted that the NVIDIA GeForce GTX 1060 uses a GP106 chip, not the GP100 chip shown in \ref{fig:pascalGPU}. 
The main difference between the two chips is the number of {CUDA Cores: The GP100 has 3840 (60 \glspl{SM}, each with 64 CUDA Cores), and the GP106 only have 1280 (20 \glspl{SM}, each with 64 CUDA cores).

We were not able to find a whitepaper or similar report from reliable sources which detailed the GP106 chip, so that is why it is not used as the example. 


\subsubsection{AMD}
For this project we have a AMD Sapphire Radeon R9 280 3GB GDDR5 \gls{GPU} at our disposal. It uses the \gls{GCN} 1.0 Architecture.
It is the first architecture AMD developed that would allow to use \glspl{GPU} as \glspl{GPGPU}.

\fig{figures/GCN_compute_unit}{Overview of the \gls{GCN} architecture}{GCNCU}{1} 

In \gls{GCN} Compute Units are the basic computational blocks of the \gls{GCN} architecture, Figure \ref{fig:GCNCU}} visualizes how each \gls{CU} is structured in \gls{GCN}.
\todo{make figure of wavefront-simd-CU relation}
A wavefront consists of 64 threads that can be executed in parallel, these wavefronts are distributed amongst \gls{CU}s. 
Each \gls{CU} consists of 4 separate SIMD units which all contain their own vector ALU, which is used for vector cal\gls{CU}lations.  
Each SIMD unit has a buffer that can hold up to 10 wavefronts, meaning that each \gls{CU} is able to work on up to 40 wavefronts in parallel during run time (therefore a single CU can run 2560 threads in parallel). 
The more \glspl{CU} a \gls{GCN} \gls{GPU} possesses the more workload parallelism it can achieve.
In order to reduce overhead and make it easier to use \gls{GCN} \glspl{GPU} as \glspl{GPGPU}, \glspl{CU} can communicate with one another through a inbuilt L2 cache, additionally this approach is faster in terms of transferring data than to rely on off-chip memory.

%formula for how many threads a GNC GPU can run in parallel: 40(a single \gls{CU}) x number of \gls{CU}â€™s x %64(wavefront). 
AMD Sapphire Radeon R9 280 3GB GDDR5 consist of 32 \glspl{CU}.
This means it can run 81.920 threads in parallel. (40 x 64 x 32)
\todo{cannot find a architecture picture of a AMD Sapphire Radeon R9 280 3GB GDDR5.}

\subsection{Intel}
Traditionally, Intel was a company that only produced \glspl{CPU}. However over time it was realized there was some performance gain to be had by incorporating inbuilt \glspl{GPU} into Intels \glspl{CPU}.
Intel introduced its first series of integrated graphics processors in 2010 and have since then kept releasing newer and more powerful ones than the last generation. 

These \glspl{CPU} are called Intel HD Graphics. 

For this project we have a HD graphics 4600 and HD graphics 520 available for usage. Which means they run on different architectures, gen8 and gen9 respectively.

Before describing the architecture of the inbuilt \gls{GPU}, the entire architecture of an Intel \gls{CPU} with a inbuilt \gls{GPU} will be presented.

\fig{figures/SoC_architecture}{System-On-a-Chip architecture of an Intel \gls{CPU} with a embedded \gls{GPU}. Figure taken from \cite{computegen9}}{IntelSoC}{1} 

The architecture is partitioned into three parts: \gls{CPU} cores, Processor Graphics (\gls{GPU}), and a ring interconnect domain. 
The SoC architecture was designed to easily change the components out so it can be used to design for other devices than only desktop computers.
The ring interconnect is vital for intel multi-core \glspl{CPU} as they communicate to each other through the ring. 
Embedded \glspl{GPU} are also connected to the rest of the Intel \gls{CPU} through this ring.
The on-die ring interconnect bus between the \gls{CPU} cores, caches, and embedded \gls{GPU}. 
Each connected component has its own interface for how it should use the ring (additionally, each \gls{CPU} core is considered as its own unique component). 
The SoC ring interconnect is a bi-directional ring which has a 32-byte wide data bus.  
When the \gls{CPU} needs to interact with off-chip systems, this is facilitated by the ring as well. 

The architecture of Intel Processor Graphics was designed to be modular in order to make it more scalable. 
This means it is possible to design Intel Graphics HD units that will work on other devices than desktop computers. 
The architecture consists of compute components called execution units.
Execution units are clustered into groups called subslices and subslices are clustered into slices. 
Each component will be further explained.

\fig{figures/Intel_ExecutionUnit}{Architecture of a execution unit, figure taken from \cite{computegen9}}{IntelEU}{1} 

\todo{use cref here}
\ref{fig:IntelEU} shows the fundamental building block of Generation 8 and 9 \gls{GPU} architecture, the execution unit. 
The figure shows that each Execution Unit contains 7 threads, and each thread contains 128 SIMD 32-bit registers.  

On each cycle an Execution Unit can issue up to four different instructions which are sourced from up to four different threads, the thread arbiter dispatches the instructions for execution.
Each Execution Unit has a pair of FPUs used for computation (They support both Integer and Floatin-point computation despite being called FPUs).

\fig{figures/Intel_SubSlice}{Architecture of a Subslice, figure taken from \cite{computegen9}}{IntelSS}{1}
In this architecture(same for both gen8 and 9), arrays of Execution Units in groups are called a subslice. A subslice contains 8 Execution Units. 
This means each subslice can run up to 56 threads simultaneously. 
each subslice has its own local thread dispatcher, which is used to partition threads into the execution units in a subslice.
The sampler is a read only memory fetch unit, used for sampling textures and images.
The dataport is used to memory operations.

\fig{figures/Intel_Slice}{Architecture of a Slice, figure taken from \cite{computegen9}}{IntelS}{1}
Subslices grouped together are called slices. For most Generation 9 Intel Graphics HD \glspl{CPU} they are grouped as three.
This means a gen8 and 9 with a slice containing three subslices can run at most 168 threads simultaneously.
Besides grouping subslices together, a slice adds additional logic for thread dispatching and a level 3 cache.

\fig{figures/Intel_Complete}{Putting each component in its place, figure taken from \cite{computegen9}}{IntelC}{1}
It is worth to note that an intel graphics HD \gls{CPU} may contain more than one slice to give a higher throughput.
The figure shows some new components in the finally assembled Intel graphics HD \gls{CPU}. The Command streamer, Global thread dispatcher and Graphics Technology Interface.
The command streamer is used to parse commands given from driver stacks. 
The Global thread dispatcher make sure to balance workload distribution across the device, it also works with the thread dispatchers that were previously mentioned when subslices were presented.
Lastly, the Graphics Technology Interface is the component that connects the Gen8 and 9 Architecture to the rest of the SoC architecture. 
It ensures that communication between the \gls{GPU} and \gls{CPU} can happen though the interconnect ring mentioned earlier.  
Additionally, if there are more than one slice present there is a bus available to make communication between slices possible. 
Architecturally, Gen8 \cite{computegen8} and Gen9 \cite{computegen9} are nearly identical. The differences are minimal and can be seen in their respective white papers. 

Intel HD graphics 520 has 1 slice partitioned into 3 subslices, this means it has 24 execution units and can run 168 threads simultanously.
Intel HD graphics 4600 has 20 execution units, which means it can up to 140 threads simultanously. 
This partially breaks the earlier mention archiecture, however this is because of Intel making use of their modular design and making changes to the their architecture as they have need of it.
