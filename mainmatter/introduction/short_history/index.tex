\section{The Short History of Graphics \arcshort{API}s}\label{sec:short_history}
\begin{sectionmeta}
	This section gives an overview of the evolution of graphics with the focus being on the \glspl{API}.
	First section will look at graphics before graphics \glsps{API}, the example given will be that of a Commodore 64.
	After that, the early competition for getting the graphics \gls{API} market is described.
	Lastly, we look at some of the more modern improvement that is made to the modern \glspl{API}.
	
	In this section, the term 3D accelerator is used interchangeably with graphics processor or \gls{GPU}, because this is the historically accurate term. 
	They all refer to hardware designed to render 3D graphics.
\end{sectionmeta}

\subsection{Early Days of Consumer Graphics}

In the early 80's, dedicated hardware for rendering images were starting to appear.
Back then, video cards were not expected to have their own video memory, and had to share memory with the \gls{CPU} \cite{wikipedia????shared}.

This was a problem because of the limited memory in the computer.
For instance, the Commodore 64 had 64 kilo bytes of memory, and supported a 320 by 200 pixels screen \cite{commodore1983commodore}.
This meant that storing a full screen bitmap with four colors would take $$320\times 200\times 2 \text{ bits} = 128,000 \text{ bits} = 16,000 \text{ bytes of memory.}$$
One quarter of the total memory for storing the bitmap.
If the image is using 16 colors then that number would use half of the memory that the Commodore 64 contains, which would leave little space for the program and I/O, which was memory mapped \cite{commodore1983commodore}.

The Commodore 64 used the MOS Technology VIC-II graphics chip \cite{commodore1983commodore}.
The chip supports a 320 by 200 pixels video resolution, 16 colors, and 16 kB memory for screen, character, and sprites \cite{commodore1983commodore}.
It could handle eight sprites per scanline, each of them 24 by 21 pixels \cite{commodore1983commodore}.

But how could it fit 320 by 200 screen with 16 colors into 16 kB of memory? It could not.

Instead, the Commodore 64 used a technique know as color cells \cite{commodore1983commodore}, which were smaller screen segments that used a limited palette.
This technique was also used by the Nintendo Entertainment System. 
The Commodore 64 had two modes for displaying color cells; high resolution mode, where all 320 by 200 pixels were usable, but only two colors, and multi-color mode, where every second pixel was a repeat of the previous \cite{commodore1983commodore}.
The benefit is that in multi-color mode you can use four colors instead of two.
This was popular for games since color was more important than resolution \cite{bogdan2014games}.
Regardless of which mode is used the screen would use eight kB of memory and palettes would use one kB memory, the rest of the memory was used for sprites \cite{commodore1983commodore}.

Sprites were a separate system that could be drawn on top of the bitmap.
The Commodore 64 supported up to eight sprites \cite{commodore1983commodore}.

Other systems such as the Nintendo Gameboy used similar methods with different restrictions \cite{nintendo1999gameboy}.
For instance, the Nintendo Gameboy supported 40 sprites, but they had to be eight by eight pixels, and only ten could be on a single scanline \cite{nintendo1999gameboy}. 
The screen was also smaller, 160 by 144 pixels, and always supported four colors, that had to be the infamous four shades of green \cite{nintendo1999gameboy}.

To program graphics on the Commodore 64, the programmer would write to and read from the 47 graphics registers in the Commodore 64s address range 53,248 to 53,294 \cite{commodore1983commodore}.
As an example, if you were to put a sprite into the top left corner of the screen after inserting the sprite data into the memory you would write the following BASIC commands; 

\begin{lstlisting}[caption={Small program that sets the coordinate of sprite 0 to (0,0) (line 1-3), sets the color to green(line 4), and displays it (line 5) (BASIC).}, language=Basic]
10 POKE 53248,0
20 POKE 53264,0
30 POKE 53249,0
40 POKE 53287,5
50 POKE 53269,1
\end{lstlisting}
This is fairly verbose, both because it is written in tha BASIC language, and because you had to write to specific hardcoded registers.
But this was the way it was done before graphics \glspl{API} became prominent.

It was first with the rise of 3D rendering that graphics \glspl{API} became a prominent part of the toolset available to developers.
And advances in 3D rendering had a benificial effect of 2D rendering effeciency.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Rise of Graphics \arcshort{API}s} 

In January of 1992 Silicon Graphics Inc released the first version of OpenGL, which tried to streamline the process of 2D and 3D graphics development \cite{segal1994opengl}.
It was a based on their own proprietary graphics libary; \gls{IRIS GL}.
The first version of OpenGL were far from perfect and some of its flaws, such as no texture objects \cite{kronos????history}.

When RenderMorphic's Reality Lab started to gain traction, Microsoft bought them in Febuary of 1995 and it became the core for Microsoft's own 3D rendering \gls{API} \cite{1997crushed}; Direct3D.
The first version of Direct3D shipped in June 1996 with DirectX 2.0 \cite{wikipedia????directx}. 

Around the same time 3dfx's Brian Hook wrote Glide, a graphics API for the 3dfx Voodoo Graphics Accelerator.
%Glide was inspired by OpenGL, but focused on features that were useful for realtime 3D rendering.
Because of the ease of programming to Glide, and the popularity of the Voodoo graphics accelerators, Glide became the dominating graphics \gls{API} in the late 1990's.

Glides success was in part made possible by Microsoft's involvement in the Talisman project, which took some of resources from the DirectX team.
The Talisman project was a new 3D architechture that could reduce the memory bandwidth needed by applying tiled rendering \cite{torborg1996talisman}.
But as other 3D accelerators became more efficient and cheaper, Microsoft cancelled project Talisman and reverted their focus to Direct3D \cite{wikipedia????talisman}.
The market share of the Windows operating system gave Microsoft the consumer base that the hardware specific \glspl{API} lacked.
So soon developers started to focus their attention on Direct3D.

\citet{carmack1996plan} from Id Software critized Direct3D for being too verbose, after trying to port Quake to OpenGL and Direct3D.
So there were some resistance to Direct3D's market dominance, so some developers insisted on using OpenGL.
And so the OpenGL and Direct3D became the dominant competitors in the 3D rendering 

\subsection{Modern Graphics \arcshort{API}s}

The early days of 3D programming was still very verbose and inflexible. 
There were using fixed function pipelines instead of shaders, which ment you could not change how the pipeline worked \cite{davidovic2014fixedfunction}.
Most of the time 
But in November 2000, Microsoft released their first version of \gls{HLSL} together with DirectX 8 \cite{wikipedia????directx}.
\Gls{HLSL} is a high level, C like language that specifies how the \gls{GPU} should render a given model.
It also served as a way to decide which calculations should be done in which render stages in the pipeline.

OpenGL followed with their own shader language \gls{GLSL} in April 2004.
Like \gls{HLSL}, \gls{GLSL} is a C like language with some extensions to better support operations common to graphics rendering.

Because OpenGL and Direct3D is in direct competition with eachother, and operate on the same hardware, once a technology, or tool has been developed for one, the other usually follows soon after.
Therefor we look at the general graphics \gls{API} improvements in the following sections.

The first improvement to the rendering pipeline was geometry shaders.
This shader was in between the vertex and fragment shader and operated on primitives; points, lines, or triangles, and returned zero or more primitives for rasterizing.
Because the geometry shader could see multiple verticies at once, developers were exited for the oppotunities this would give.
A good example of using the geometry shader is particle effects, where the specifics of the four corners of the mesh is less important for the developer than the position of the particle.
So instead, the developer can send the points of the particles to the \gls{GPU}, and have the geometry shader generate the corners in parallel.

The next change to the rendering pipelin was the introduction of a tesselation stage.
This stage involves two shaders; a hull shader and a domain shader.
It occurs after the vertex shader, but before the geometry shader.
The tesselation stage generates more details to the model by adding even more vertices.
Where the vertices are placed and how many there are, is handled by the shaders.

Later on, tesslation became the big topic, which is another way to generate complexity on the \gls{GPU}.
The tesselation phase is a way to fix some of the promises that the geometry shader promised but never delivered on.

The most recent trend is to lower the abstraction level of the \glspl{API} to enable more customization.
This push can be seen in the most recent version of Direct3D; Direct3D 12, and the so called successor to OpenGL; Vulkan.
The lower level abstract enables the developer to customize the \gls{GPU} usage to their or their games needs.
Removing this layer of abstraction also has the benefit of enabling multi-threaded \gls{GPU} calls, by eliminating the driver state.
