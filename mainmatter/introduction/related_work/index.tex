\section{Related Works}\label{sec:related_works}
During the search for related works within this project, it was discovered that we were hardly the first oncs to wonder, how graphics \glspl{API} function. From these papers it was discovered that a \gls{GPU} is used for two kinds of operation: as a GP\gls{GPU}, meaning using the \gls{GPU} for general purpose programing and utilizing its massive parallelism. 
The second purpose for the \gls{GPU} is for rendering graphics, this is mostly used in video game software. The following sections will further explain the related works, and why they are relevant.

\subsection{GP\gls{GPU}}
This section will describe related work as to what GP\gls{GPU} are, what they are used for, and how they relate to the project.

The GPU Computing Era \cite{gpu_computing_era} is an article, which discusses the benefits of utilizing \gls{GPU} parallarism to run applications that previously were deemed too time consuming to use in practice. 
The article claims that single-threaded applications are no longer good enough, and the industry should adapt to GP\gls{GPU} technology. 
It also mentions, how the \gls{GPU} becomes more powerful all the time by doubling up on its transistors for every 18th month. 
One way of utilizing GP\gls{GPU} is through CUDA.
CUDA programs are very scalable according to the article, and it is thus an excellent tool to utilize GP\gls{GPU} functionality, and encourages more ussage of GP\gls{GPU}. It is important to note that the authors of this article are from Nvidia, and they are likely biased towards how important GP\gls{GPU} is, and how good their CUDA framework tool is. 

This article is relevant as it gives an introduction to how a \gls{GPU} might work, it specifically mentions how the Fermi architecture works, and how it is possible to achieve parallarism.

There have been made a number of tools in order to make utilization of GPGU easier, these Higher Level GP\gls{GPU}'s tool includes the following: Firepile (Scala) \cite{2011_firepile}, OCaml GP\gls{GPU} \cite{bourgoin_2017_high}, PyCuda and PyOpenCL \cite{2012_pycuda_pyopencl} and Chestnut \cite{stromme_2012_chestnut}.

These tools were made in order to make it easier and less error prone to utilize GP\gls{GPU} for less skilled developers. As it can be difficult to work with OpenCL and CUDA alone.
The value in these articles lies in how the developers expose lower level \glspl{API} to a custom made higher level \gls{API}, and whether or not they are easier to use than their lower-level counterparts. Addtionally, it is also interesting to see how well these higher level abstraction compare performance-wise to the lower level ones. However, a pitfall the four papers has is that they do not have a proper methodology to test out the programmability of these \glspl{API}, which is highly desired when the goal of these tools is to be easier and more intuitive to use than their lower-level counterparts.

Designing efficient sorting algorithms for Manycore \glspl{GPU} \cite{satish_2009_designing} is an article describes the development of a custom implemented radix sort and a merge sort, and prove the capabilites of CUDA and GPGPU parallarism, by running several Nvidia \glspl{GPU} for comparrison. 
This article demonstrates how much throughput there is to be gained from GP\glspl{GPU}, and how a comparison can be done. 
A critique of the paper though is that mege sort and radix sort starts to show strange performance spike patterns when working with large workload. 
This is never explained as to why, it should have been elaborated.
Or at the very least hazard a guess.

Debunking the 100x \gls{GPU} vs. CPU Myth \cite{lee_2010_debunking} claims that GP\glspl{GPU} are not that much better than \glspl{CPU}. 
It references several papers that claim \glspl{GPU} can be 100 (or more) times better than a \gls{CPU}, and attempts to debunk them. 
With the data that the article collects it concludes that \glspl{GPU} are only 2x times better on average than the \glspl{CPU}. 
The testing was done by writing several algorithms and implement them for both the \gls{GPU} and \gls{CPU}, running the algorithms, observing the performance, and then comparing the results. 
However the \gls{CPU} implementation is highly optimized as the authors and software writers are from Intel, whilst the GP\gls{GPU} implementation for the algorithms is not optimized to its fullest. 
Data from an article like this would have been more meaningful if the GP\gls{GPU} implementation was written some of the best people from Nvidia (since they tested with a Nvidia card) instead of someone from Intel.

\subsection{Graphics}
This section will describe how \glspl{GPU} usually are used in terms of rendering graphics, and elaborate on what kind of issues that can be encountered whilst working with rendering graphics.

Let's Fix OpenGL \cite{fix_opengl} is a journal article that attempts to expose the shortcommings of OpenGL, and suggests what can be done in order to make OpenGL better. 
The issues mentioned in the article also applies to DirectX according to the author. 
The article identifies six issues: 
\paragraph 1 Programmers must juggle between C/C++ and HLSL and GLSL.
Meaning they have to switch between coding the renderer and coding the shaders in different programming languages.

\paragraph 2 The communication between CPU and \gls{GPU} is brittle. 
They communicate through commands written in the application, which is a very error-prone approach.

\paragraph 3 Lack of meta-programming tools when it comes to shaders. 
Everything must be explictly stated in the shader programs.

\paragraph 4 Different semantics for each shader stage. 
There are different tools for. Contributes to the issue of having to keep track of semantics depending on what kind of shader is being worked on.

\paragraph 5 No type system for vectors when converting between spaces. 
It has to be done manually.

\paragraph 6 Difficlt to verify correctness of a graphics application. 
There is no way to test whether whatever an application renders, is the actual desired result.

In the conclusion, the article mentions Vulkan and that it might be the solution to the issues of OpenGL. 
Additionally, it also encourages development of new frameworks to rival OpenGL. 
However a shortcomming of the article is that all the listed issues is only based on the authors opinion that he has discussed with his collegues, and not used a proper methodology to collect this data. 

The value of this article is that it gives insight as to how a good graphics \gls{API} should work, it's good arguments to take into consideration when trying to evaluate an \gls{API}.

An Incremental Rendering VM \cite{haaser_2015_incremental} 

Reducing Driver Overhead in OpenGL, Direct3D and Mantle  \cite{dobersberger_2015_reducing}

\subsection{Comparrison}
There are several options in terms of what \gls{GPU} \gls{API} that can be used when a developer needs to render graphics in an application. This section discusses articles that compare \gls{GPU} \glspl{API} against one another, and why and when certain \glspl{API} should be chosen over others.

Direct3D 11 vs 12 A Performance Comparison Using Basic Geometry \cite{2016_direct3d} is a master thesis...


Evaluation of multi-threading in Vulkan \cite{blackert_2016_evaluation} is a master thesis that attempts at evaluating the multi-threading performance of Vulkan. It does so by comparing it to its predescor; OpenGL. 
Additionally it also evaluates the programmability of Vulkan. 
In the conclusion, the thesis states that Vulkan can give more throughput than OpenGL, although it is worth noting that the performance increase is dependent on what kind of hardware is used. Additionally not all applications will gain a significant performance boost with using Vulkan over OpenGL. 
This would be in cases where multi-threading is not needed, or if the application is not CPU bound. No methodology was used in evaluating the programmability of Vulkan, it is based on the personal experience of the author, which is not a good enough method to evaluate an \gls{API} on. 
It only states that Vulkan is more difficult to work with than OpenGL, because there is more overhead. 
For future work the thesis encourages to further evaluate the performance of Vulkan multi-threading  capabilities by comparing it to Direct3D 12, and testing the portability of Vulkan on various operating systems and different \gls{GPU} manufactoreres.

\paragraph{}
From these sources it is possible to form a better overview of what is currently going on in the area of graphics in the scientific community. Based on these sources, it can be concluded that there are not many articles that are looking into the programmability of \gls{GPU} \glspl{API}, and the thesis papers evaluation method is flawed as they typically compare a low-level \gls{API} like Vulkan with a higher level one, and do nothing to counter the abstraction level mismatch between the \glspl{API}.

Aditionally there seems to be a lack on papers discussing Direct3D 12 and Vulkan(Besides the master thesis' there are none), and should therefore be considered a field that is worthwhile looking into.
