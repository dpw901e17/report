\section{Related Works}\label{sec:related_works}
Having listed the relevant background knowledge regarding our work, we wrap up this chapter by discussing some related publications.
This is done as to put our own efforts in an academic context. 


\subsection{Graphics Programmability}
Research focusing on the programmability of graphics programming tools is very sparse.
Programmability research seem to focus on more widely used technologies such as general-purpose programming languages.
This subsection will present a few of the publications that discuss programmability in the field of graphics programming. 


\subsubsection{Let's Fix OpenGL}
\citet{fix_opengl} is a critique of modern OpenGL and by extension Direct3D.
The author has not been extensivly involved with graphics programming, so this publication provides an outsider’s perspective.
Although it should be noted that the statements discussed are based on the author’s personal experiences and not any empiric research. 
As opposed to our work, which focuses strictly on the graphics \gls{API}, the critique focuses mostly on shader programming.


The first issue presented, is that C-like shader languages are not clean extensions of either C nor C++.
As an example, from \gls{GLSL}, the type declared by struct t \{…\} is referred to as t like in C++, not as struct t like in C.
Yet as in C, variables cannot be declared within if statements. This means that programmers need to be aware of the subtle differences between host and shader code.

\fig{figures/openglShaders.pdf}{Example vertex and fragment shader in \gls{GLSL}.\cite{fix_opengl}}{openglShaders}{1}
  
\cref{fig:openglShaders} presents some \gls{GLSL} code for both a vertex and fragment shader.
These will either be represented by strings in the host code or in separate files.
Even though \gls{GLSL} is a strongly typed language, errors will only be found when the shaders are compiled during execution of the host code.
This brings up the issue that the compiler can not make any static analysis, of how the host code communicates with the shaders, or how the shaders communicate with each other.


The shaders in \cref{fig:openglShaders} communicate using the fragPos variable, but if this name does not match in the two programs, we are only informed of this error at runtime. 
While the shaders in \cref{fig:openglShaders} are rather simple, a graphics application often needs many different types of shaders.
To make the use of shaders flexible, metaprogramming is used to combine snippets of \gls{GLSL} code as to construct the needed shaders. 
This is done using the \#define and \#ifdef directives, but as the amount of shaders needed scales up the programs created becomes unmanageable.


The different kinds of shader stages have different execution semantics, which can make it difficult to understand how the stages relate to each other.
For instance, the fragment shader runs multiple times for each vertex shader invocation.
In addition, values passed between the two stages are interpolated during rasterization.
In the case of \cref{fig:openglShaders}, the fragPos variable will include the vertex position in the vertexShader, but fragPos in the fragment shader represents the interpolation of several vertex positions.

\fig{figures/openglLighting.pdf}{Excerpt from vertex shader implementing the phong lighting model. \cite{fix_opengl}}{lighting}{1}

The vertex shader by itself could also benefit from better static type checking. In this shader we convert points in model space to world space, then to camera space and finally to projection space.
This creates four versions of the same point in different spaces.
For some lighting algorithms, we need to perform operations on a point while it is in a certain space. 
This is depicted in \cref{fig:lighting}, where we need to compute the angle of light hitting the vertex by subtracting its position in world space from the light position, which is also in world space. 
It would not make sense to mix the spaces by using the vertex position in camera space instead, but no system is in place to ensure that this does not occur.

\fig{figures/teapot.pdf}{Buggy and non-buggy output of the phong lighting model.\cite{fix_opengl}}{shaderBug}{1}   

The lack of such type checks meant that the author himself mixed spaces, while writing his shader.
However, this bug was not immediately apparent as can be seen from \cref{fig:shaderBug} which compares the shader with and without this bug.
This brings up the difficulty of determining the correctness of graphics programs in general.
No tools exist yet to assist in this rather complex task. 
 
 
Considering these issues, the author calls for a better \gls{API} to be written, which can rival OpenGL and Direct3D.
It is suggested that this could be built on top of Vulkan, as this is more of a hardware abstraction than OpenGL itself.
While only an opinion piece, it does describe a handful of relevant issues that impact programmability of modern \glspl{API}.


\subsubsection{An Incremental Rendering VM} 
\citet{haaser_2015_incremental} does not focus on programmability explicitly, but instead on how to design efficient middleware between a high-level \glspl{API} like OpenSceneGraph and a low-level \glspl{API} like OpenGL. 


During rendering, we can increase performance by minimizing how often we switch between render stages.
For this publication, the authors only concern themselves with the parts of the render state, which are most costly to change.
These are the shaders, textures and buffers used when drawing an object with a draw call.
In an OpenGL or Direct3D program, the programmer takes advantage of this by manually sorting the objects to be rendered in such a way that the render state changes infrequently.
Computing the render order is done for each frame. 


The authors observe that most of the draw calls used for rendering one frame, can be reused for the next frame with great likelihood.
This means that we do not have to recompute the render order for each frame. 

\fig{figures/trie.pdf}{Example of a trie, used to manage sorted draw call code.\cite{haaser_2015_incremental}}{trie}{1}

To take advantage of this, they construct an incremental rendering \gls{VM}, which holds onto executable rendering code between frames, and only updates it when needed.
For this they use a trie as shown in \cref{fig:trie}, which is used to sort objects into different state buckets at the trie’s leafs.
Each object has some memory allocated for the virtual machine code, which uses OpenGL to draw it on screen. 
Code blocks of objects in the same bucket are rendered sequentially, as to only change states, when we start drawing objects from another bucket. 


The \gls{VM} initially takes this trie and compiles it into virtual machine code, which is placed in a continuous part of memory.
Both the trie and the code are retained between frames. 


As the scene changes, the trie must be updated.
Incremental changes impact how an object in the trie is to be rendered.
If this affects the render state to be used, the object may need to be placed in another bucket.
Structural changes occur when objects are added or removed from the scene, meaning that insertion and removal from the trie must be performed.
No matter the change, when a new object lands in a bucket, its executing code is compiled in a \gls{JIT} manner and inserted into a newly allocated part of memory.
As these changes occur, memory for the code blocks must be allocated and deallocated. 
This leads to the memory becoming fragmented.
As this impacts performance negatively because of cache misses, the \gls{VM} also makes sure to defragment its memory occasionally.

 
At the end of the publication, this system is benchmarked against a handful of direct OpenGL and Direct3D 11 implementations. 
This shows that the system introduces no significant overhead and outperforms implementations with no state sorting. 
Thus, high-level \glspl{API} do not need to underperform, if they use good middleware to communicate with the underlying low-level \gls{API}. 
While we are looking at low level \glspl{API} in this report, this does not mean that the field of high-level graphics \glspl{API} is not interesting either. 
Especially if it can still yield good performance.


\subsection{Comparison}
This subsection will discuss publications that compare modern graphics \glspl{API}.
This work is very much related to our own. Each paper is written as a master thesis and has recently been published.
They mostly focus on benchmarking the performance of applications written in different \glspl{API} and comparing them.
From what we have found, it does not seem that any in-depth comparison of Direct3D 12 and Vulkan has been published. 


\subsubsection{Reducing Driver Overhead in OpenGL, Direct3D and Mantle}
\citet{dobersberger_2015_reducing} brings up how Mantle, at the time, was said to remove driver overhead. 
The author sets out to examine the actual impact of driver overhead, and whether this is important to account for.
This is done through a comparison of OpenGL and Direct3D 11 and how their performance changes, when using new features for limiting driver overhead.
Mantle does not itself figure in the comparison, as it is not released to the public.


Some of the features tested, such as instancing, appear in both \glspl{API}.
Instancing allows for the same model to be drawn several times on screen with a single draw call.
OpenGL’s Multi draw indirect and Direct3D 11’s deferred context are features comparable to each other.
These allow for rendering commands to be stored before execution, opening the door for multithreaded command submission.
The publication also looks at a feature unique to OpenGL, Direct State Access.
Modern OpenGL allows direct access to buffers and textures without binding them.
In older versions, the programmer was forced to switch states, if they wanted to operate on a texture or a buffer.


To test these features, a base application is made, where both \glspl{API} are made to render many cubes on a screen.
The features being researched are then enabled or disabled for different tests, performance benchmarks are made, and feature comparisons are made where applicable.
A few tests using third-party graphics software are also made.


The author finds these new features, which limit driver overhead, introduce a significant amount of additional performance to graphics applications.
Using features like instancing, and multithreaded command submission we see a performance increase by an order of magnitude.


At the end of the text, the author states how features such as these will be central in the design of Direct3D 12 and Vulkan.
Thus, this is relevant to our own efforts, where we consider the performance of both \glspl{API}.

\subsubsection{Direct3D 11 vs 12 A Performance Comparison Using Basic Geometry}
\citet{2016_direct3d} presents a benchmark study comparing the performance of Direct3D 11 and Direct3D 12.
This was done by implementing a scene of colored points in both \glspl{API}. 
They look at how the \gls{FPS} evolves as the number of points to be rendered is increased.
This test is done several times, using a different number of threads for submitting commands to the \gls{GPU}. 
The author opts to model the Direct3D 12 version of the test application after the Direct3D 11 version, not using additional features provided by the newer version. 
The results of testing contradicts previous benchmarks, which show an improvement in performance when using Direct3D 12.
Direct3D 12 only gets better performance when rendering a low number of points and making a single draw call per thread.

The author  notes how the graphics driver seems to allocate more memory and threads for optimization in the case of Direct3D 11.
This is not the case for Direct3D 12, where the programmer is tasked with more of the optimization as to limit driver overhead.


In general, we believe this publication to be problematic, as Direct3D 12 is not used to its full extend.
In our work we strive to make a fairer comparison of Direct3D and Vulkan with more equivalent applications.

\subsubsection{Evaluation of multi-threading in Vulkan}
\citet{blackert_2016_evaluation} presents a comparison of OpenGL and Vulkan, being a publication like \cite{2016_direct3d}.
It focuses on looking at the performance yielded by multithreaded command submissions in Vulkan as well as state switching.
For this a test application like the one found in \cite{dobersberger_2015_reducing} is written using both OpenGL and Vulkan.
A difference being that the models rendered can be either low-detail cubes or high detail teapots.


Looking at multithreading, the author experienced a 69\% performance increase with multithreaded Vulkan, when using four threads to render a lot of low-detail cubes.
Yet, when rendering higher-detail models, OpenGL seemed to get the upper hand.
Increasing the number of threads in Vulkan did not affect the performance a lot in this case.
This is most likely because the application becomes \gls{GPU}-bound when switching to models of higher detail.
Thus, we see how Vulkan seems to be most useful for CPU-bound applications.


They also look at the change in performance, when changing the render state for each object to be rendered.
In the case of Vulkan, render state objects are defined at compile time.
This should make it faster to switch states, as opposed to OpenGL where state switching has a larger associated runtime cost.
However, when changing only some parts of the state, like enabling and disabling the culling of faces, OpenGL performs faster by 63\%.
Yet, when also making heavier state changes with more associated safety checks, like changing the vertex shader, Vulkan performs 160\% faster.


After testing, the author also brings up the programmability of Vulkan. 
He mentions how the removal of driver assistance, means that a lot more work must be done by the programmer. 
For instance, when updating a \gls{GPU} resource, you cannot use an \gls{API} call, instead you have to memory map the resource onto the \gls{CPU} and memcpy data into it yourself. 
Yet, it is also conceited that once the programmer has an initial code base set up and starts writing their own wrappers, this task becomes less daunting.


Our work differs from this publication, as we place more of a focus upon \gls{API} programmability.
Here it is discussed as opinions of the author, rather than any deep empirical work being presented.

\subsection{\gls{GPGPU}}
This subsection discusses the related field of \gls{GPGPU} programming.
As mentioned in \cref{sec:short_history}, during the 00’s \glspl{GPU} evolved into highly parallel devices with general purpose computation units.
This lead programmers to use tools like Direct3D and OpenGL for solving embarrassingly parallel problems, which were not related to rendering images to screen.
In 2007 Nvidia introduced the CUDA programming model for use with C, C++ and Fortran, followed up by Khronos with the similar OpenCL model in 2009.


\citet{gpu_computing_era} is a publication from Nvidia, explaining the benefits of \gls{GPGPU} programming and CUDA itself. 
CUDA allows for both regular sequential programming, but also supports writing procedures, kernels, to be run in parallel on the \gls{GPU}.
In a way they are like shaders, but the programmer must specify the amount of threads needed for execution.
Furthermore, these threads must be separated into thread blocks.
Threads within the same block can synchronize via local shared memory, and threads within different blocks must communicate through global memory, which is slower.
This is an instance of programmers needing to be very familiar with the \gls{GPU} architecture to get optimal performance out of \gls{GPGPU}. 


Nvidia’s article claims to get 100x speedups in performance for some problems, when compared to regular multithreading on the \gls{CPU}.
Yet, later we see Intel attempting to debunk these numbers in \cite{lee_2010_debunking}.
This publication shows benchmarks, where \gls{GPGPU} programming only yields a 2.5x speedup on average.


Even with some discussion of the performance benefits of \gls{GPGPU}, we see research into creating efficient \gls{GPGPU} algorithms. 
For instance, \cite{satish_2009_designing} presents both a radix sort and a merge sort, which outperform other \gls{GPGPU} sorting algorithms.


A particularly active area of \gls{GPGPU} has been the development of new programming tools. 
This includes language extensions such as OpenACC, where the programmer simply tags parts of their code for \gls{GPGPU} execution \cite{wienke2012openacc}.
A handful of libraries have also been released including Firepile for Scala \cite{2011_firepile}, OCaml GP for OCaml\cite{bourgoin_2017_high}, as well as PyCuda and PyOpenCL for Python\cite{2012_pycuda_pyopencl}.
We even see entire languages for \gls{GPGPU} programming being developed with Chestnut \cite{stromme_2012_chestnut}.
Note that the tools presented vary in their state of development and popularity.
Common to all, is that the publications introducing them contain no empiric evidence for their programmability.
This is a problem as most of them claim to be easier to use than the low level CUDA and OpenCL tools.
Their programmability is mostly backed up by the author’s own claims, sometimes backed up by the showcase of some example programs. 
