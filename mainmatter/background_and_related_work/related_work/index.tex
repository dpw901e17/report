\section{Related Works}\label{sec:related_works}
Having listed the relevant background knowledge regarding our work, we wrap up this chapter by discussing some related publications.
This is done as to put our own efforts in an academic context. 


\subsection{Graphics Programmability}
Research focusing on the programmability of graphics programming tools is very sparse.
Programmability research seem to focus on more widely used technologies such as general-purpose programming languages.
This subsection will present a few of the publications that discuss programmability in the field of graphics programming. 


\subsubsection{Let's Fix OpenGL}
\citet{fix_opengl} is a critique of modern OpenGL and by extension Direct3D.
The author has not been extensivly involved with graphics programming, so this publication provides an outsider’s perspective.
Although it should be noted that the statements discussed are based on the author’s personal experiences and not any empiric research. 
As opposed to our work, which focuses strictly on the graphics \gls{API}, the critique focuses mostly on shader programming.


The first issue presented, is that C-like shader languages are not clean extensions of either C nor C++.
As an example, from \gls{GLSL}, the type declared by struct t \{…\} is referred to as t like in C++, not as struct t like in C.
Yet as in C, variables cannot be declared within if statements. This means that programmers need to be aware of the subtle differences between host and shader code.

\fig{figures/openglShaders.pdf}{Example vertex and fragment shader in \gls{GLSL}.\cite{fix_opengl}}{openglShaders}{1}
  
\cref{fig:openglShaders} presents some \gls{GLSL} code for both a vertex and fragment shader.
These will either be represented by strings in the host code or in separate files.
Even though \gls{GLSL} is a strongly typed language, errors will only be found when the shaders are compiled during execution of the host code.
This brings up the issue that the compiler can not make any static analysis, of how the host code communicates with the shaders, or how the shaders communicate with each other.


The shaders in \cref{fig:openglShaders} communicate using the fragPos variable, but if this name does not match in the two programs, we are only informed of this error at runtime. 
While the shaders in \cref{fig:openglShaders} are rather simple, a graphics application often needs many different types of shaders. To make the use of shaders flexible, metaprogramming is used to combine snippets of \gls{GLSL} code as to construct the needed shaders. This is done using the \#define and \#ifdef directives, but as the amount of shaders needed scales up the programs created becomes unmanageable.


The different kinds of shader stages have different execution semantics, which can make it difficult to understand how the stages relate to each other.
For instance, the fragment shader runs multiple times for each vertex shader invocation.
In addition, values passed between the two stages are interpolated during rasterization.
In the case of \cref{fig:openglShaders}, the fragPos variable will include the vertex position in the vertexShader, but fragPos in the fragment shader represents the interpolation of several vertex positions.

\fig{figures/openglLighting.pdf}{Excerpt from vertex shader implementing the phong lighting model. \cite{fix_opengl}}{lighting}{1}

The vertex shader by itself could also benefit from better static type checking. In this shader we convert points in model space to world space, then to camera space and finally to projection space.
This creates four versions of the same point in different spaces.
For some lighting algorithms, we need to perform operations on a point while it is in a certain space. 
This is depicted in \cref{fig:lighting}, where we need to compute the angle of light hitting the vertex by subtracting its position in world space from the light position, which is also in world space. 
It would not make sense to mix the spaces by using the vertex position in camera space instead, but no system is in place to ensure that this does not occur.

\fig{figures/teapot.pdf}{Buggy and non-buggy output of the phong lighting model.\cite{fix_opengl}}{shaderBug}{1}   

The lack of such type checks meant that the author himself mixed spaces, while writing his shader.
However, this bug was not immediately apparent as can be seen from \cref{fig:shaderBug} which compares the shader with and without this bug.
This brings up the difficulty of determining the correctness of graphics programs in general.
No tools exist yet to assist in this rather complex task. 
 
 
Considering these issues, the author calls for a better \gls{API} to be written, which can rival OpenGL and Direct3D.
It is suggested that this could be built on top of Vulkan, as this is more of a hardware abstraction than OpenGL itself.
While only an opinion piece, it does describe a handful of relevant issues that impact programmability of modern \glspl{API}.


\subsubsection{An Incremental Rendering VM} 
\citet{haaser_2015_incremental} does not focus on programmability explicitly, but instead on how to design efficient middleware between a high-level \glspl{API} like OpenSceneGraph and a low-level \glspl{API} like OpenGL. 


During rendering, we can increase performance by minimizing how often we switch between render stages.
For this publication, the authors only concern themselves with the parts of the render state, which are most costly to change.
These are the shaders, textures and buffers used when drawing an object with a draw call.
In an OpenGL or Direct3D program, the programmer takes advantage of this by manually sorting the objects to be rendered in such a way that the render state changes infrequently.
Computing the render order is done for each frame. 


The authors observe that most of the draw calls used for rendering one frame, can be reused for the next frame with great likelihood.
This means that we do not have to recompute the render order for each frame. 

\fig{figures/trie.pdf}{Example of a trie, used to manage sorted draw call code.\cite{haaser_2015_incremental}}{trie}{1}

To take advantage of this, they construct an incremental rendering \gls{VM}, which holds onto executable rendering code between frames, and only updates it when needed.
For this they use a trie as shown in \cref{fig:trie}, which is used to sort objects into different state buckets at the trie’s leafs.
Each object has some memory allocated for the virtual machine code, which uses OpenGL to draw it on screen. 
Code blocks of objects in the same bucket are rendered sequentially, as to only change states, when we start drawing objects from another bucket. 


The \gls{VM} initially takes this trie and compiles it into virtual machine code, which is placed in a continuous part of memory.
Both the trie and the code are retained between frames. 


As the scene changes, the trie must be updated.
Incremental changes impact how an object in the trie is to be rendered.
If this affects the render state to be used, the object may need to be placed in another bucket.
Structural changes occur when objects are added or removed from the scene, meaning that insertion and removal from the trie must be performed.
No matter the change, when a new object lands in a bucket, its executing code is compiled in a \gls{JIT} manner and inserted into a newly allocated part of memory.
As these changes occur, memory for the code blocks must be allocated and deallocated. 
This leads to the memory becoming fragmented.
As this impacts performance negatively because of cache misses, the \gls{VM} also makes sure to defragment its memory occasionally.

 
At the end of the publication, this system is benchmarked against a handful of direct OpenGL and Direct3D 11 implementations. 
This shows that the system introduces no significant overhead and outperforms implementations with no state sorting. 
Thus, high-level \glspl{API} do not need to underperform, if they use good middleware to communicate with the underlying low-level \gls{API}. 
While we are looking at low level \glspl{API} in this report, this does not mean that the field of high-level graphics \glspl{API} is not interesting either. 
Especially if it can still yield good performance.


\subsection{Comparison}
This subsection will discuss publications that compare modern graphics \glspl{API}.
This work is very much related to our own. Each paper is written as a master thesis and has recently been published.
They mostly focus on benchmarking the performance of applications written in different \glspl{API} and comparing them. 
Programmability does not seem to be a big focus, although it is occasionally mentioned in passing. 


\subsubsection{Direct3D 11 vs 12 A Performance Comparison Using Basic Geometry}
\citet{2016_direct3d} presents a benchmark study comparing the performance of Direct3D 11 and Direct3D 12.
This was done by implementing a scene of colored points in both \glspl{API}. 
They look at how the \gls{FPS} evolves as the number of points to be rendered is increased.
This test is done several times, using a different number of threads for submitting commands to the \gls{GPU}. 
The author opts to model the Direct3D 12 version of the test application after the Direct3D 11 version, not using additional features provided by the newer version. 
The results of testing contradicts previous benchmarks, which show an improvement in performance when using Direct3D 12.
Direct3D 12 only gets better performance when rendering a low number of points and making a single draw call per thread.

The author  notes how the graphics driver seems to allocate more memory and threads for optimization in the case of Direct3D 11.
This is not the case for Direct3D 12, where the programmer is tasked with more of the optimization as to limit driver overhead.


In general, we believe this publication to be problematic, as Direct3D 12 is not used to its full extend.
In our work we strive to make a fairer comparison of Direct3D and Vulkan with more equivalent applications.


\subsubsection{Reducing Driver Overhead in OpenGL, Direct3D and Mantle}

\citet{dobersberger_2015_reducing} covered three different graphics \gls{API} as part of his master thesis. 
He looked at the different ways the \glspl{API} enable the developer to reduce the overhead of using their \gls{API}. 
He then uses that knowledge to conclude something about the individual \glspl{API} such as; Direct3D focuses on multithreaded rendering, and Mantle's low-level api reduces \gls{CPU} usage and memory overhead.

Because Mantle is not released to the public, only OpenGL and Direct3D 11 are compared.
They are compared by targeting some of the features described in the specific \gls{API} sections,\todo{What sections? Ours or his?} and mesuring the performance when using the feature and when ignoring them.
If there exist comparable features in both \glspl{API}, then the performance tests are grouped together, to compare the different approaches to the problem.
In some cases, the results are verified by using third party \gls{API} tests.

The final conclusion states that overhead \glspl{API} introduces is significant, and that developers should be aware of the trade-off they make when using one feature over another.
Often the trade-off is flexibility for performance.
The author is optimistic about Vulkan and Direct3D 12, as some of the features proposed are gonna reduce the overhead.

It is releavant to this project \todo{vend det med de andre om vi kalder det et projekt} since both Vulkan and Direct3D 12 tries to reduce the \gls{API} overhead.
One way that this tries to achive this is by making the developer responsible for defining the pipeline.
We could continue his work by comparing specific features of Vulkan and Direct3D 12 on how they work and how much they reduce the \gls{API} overhead, if at all.

\subsubsection{Evaluation of multi-threading in Vulkan}

Evaluation of multi-threading in Vulkan \cite{blackert_2016_evaluation} is a master thesis that attempts at evaluating the multi-threading performance of Vulkan. It does so by comparing it to its predescor; OpenGL. 
Additionally it also evaluates the programmability of Vulkan. 

In the conclusion, the thesis states that Vulkan can give more throughput than OpenGL. \todo{A bit weird how we go in a lot of detail with other papers, while this is described in a rather short form. Perhaps find a middleground in detail and stick with it.}
Not all applications will gain a significant performance boost with using Vulkan over OpenGL. 
This would be in cases where multi-threading is not needed, or if the application is not CPU bound. 
No methodology was used in evaluating the programmability of Vulkan, it is based on the personal experience of the author, which is not a good enough method to evaluate an \gls{API} on. 
It only states that Vulkan is more difficult to work with than OpenGL, because there is more overhead since Vulkan is at a lower abstraction level than OpenGL. 

For future work the thesis encourages to further evaluate the performance of Vulkan multi-threading  capabilities by comparing it to Direct3D 12, and testing the portability of Vulkan on various operating systems and different \gls{GPU} manufactoreres.
\todo{Should we do a critique of them comparing very different APIs?}

\subsection{\gls{GPGPU}}
This section will describe related work as to what GP\gls{GPU} are, what they are used for, and how they relate to the project.

Designing efficient sorting algorithms for Manycore \glspl{GPU} \cite{satish_2009_designing} is an article which describes the development of a custom implemented radix sort and a merge sort, and prove the capabilites of CUDA and \glspl{GPGPU} parallarism, by running several Nvidia \glspl{GPU} for comparrison. 
This article demonstrates how much throughput there is to be gained from \glspl{GPGPU}, and how a comparison can be done. 
A critique of the paper though is that mege sort and radix sort starts to show strange performance spike patterns when working with large workload. 
This is never explained as to why, it should have been elaborated or at the least given a guess.
The article demonstrates how powerful the \gls{GPU} is when it is able to do some things better than a \glspl{CPU}.

The GPU Computing Era \cite{gpu_computing_era} is an article, which discusses the benefits of utilizing \gls{GPU} parallarism to run applications that previously were deemed too time consuming to use in practice. 
The article claims that single-threaded applications no longer perform well enough up against multi-threaded applications, and the industry should adapt to GP\gls{GPU} technology. 
It also mentions, how the \gls{GPU} becomes more powerful all the time by doubling up on its transistors for every 18th month. 
One way of utilizing \gls{GPGPU} is through CUDA. (Nvidia implementation of a gls{GPGPU} gls{API}.)
CUDA programs are very scalable according to the article, and it is thus an excellent tool to utilize \gls{GPGPU} functionality, and encourages more ussage of \gls{GPGPU}.
It is important to note that the authors of this article are from Nvidia, and they are likely biased towards how much power a \gls{GPGPU} can provide, and how good CUDA is. 

This article is revevant as it shows an alternative use of the \gls{GPU}, it is also explains how it achieves the parallarism which contributes to a better \gls{GPU} throughput than a \gls{CPU} can provide.

There have been made a number of tools in order to make utilization of GPGU easier, these Higher Level GP\gls{GPU}'s tool includes the following: Firepile (Scala) \cite{2011_firepile}, OCaml GP\gls{GPU} \cite{bourgoin_2017_high}, PyCuda and PyOpenCL \cite{2012_pycuda_pyopencl} and Chestnut \cite{stromme_2012_chestnut}.
It is important to mention that these \glspl{API} are only some of the tools that provide higher abstraction level GPGPU programming. 
Additionally these tools are mostly academeic experiements.

These tools were made in order to make it easier and less error prone to utilize GP\gls{GPU} for less skilled developers.
As it can be difficult to work with the low level \gls{API} (eg. CUDA).
The value in these articles lies in how the developers expose lower level \glspl{API} to a custom made higher level \gls{API}, and whether or not they are easier to use than their lower-level counterparts. 
Addtionally, it is also interesting to see how well these higher level abstraction compare performance-wise to the lower level ones. 
However, a pitfall the four papers has is that they do not have a proper methodology to test out the programmability of these \glspl{API}, which is highly desired when the goal of these tools is to be easier and more intuitive to use than their lower-level counterparts.
The articles attempts to show the programmability of their \glspl{API} by exemplifying what kind of issues their \gls{API} can solve, the syntax of using the \gls{API} and sometimes with a performance evaulation.
However it all comes down to just the authors opinion.

Debunking the 100x \gls{GPU} vs. \gls{CPU} Myth \cite{lee_2010_debunking} claims that GP\glspl{GPU} are not that much better than \glspl{CPU}. 
It references several papers that claim \glspl{GPU} can be 100 (or more) times better than a \gls{CPU}, and attempts to debunk them. 
With the data that the article collects it concludes that \glspl{GPU} are only 2x times better on average than the \glspl{CPU}. 
The testing was done by writing several algorithms and implement them for both the \gls{GPU} and \gls{CPU}, running the algorithms, observing the performance, and then comparing the results. 
However the \gls{CPU} implementation is highly optimized as the authors and software writers are from Intel, whilst the GP\gls{GPU} implementation for the algorithms is not optimized to its fullest. 
Data from an article like this would have been more meaningful if the GP\gls{GPU} implementation was written some of the best people from Nvidia (since they tested with a Nvidia card) instead of someone from Intel.

\paragraph{}
From these sources it is possible to form a better overview of what is currently going on in the area of \glspl{GPU} in the scientific community. 
Based on these sources, it can be concluded that not many articles are looking into the programmability of \gls{GPU} \glspl{API}. 
The thesis papers were the only once that made an attempt at evaluating Vulkan and DirectX12.

Aditionally there seems to be a lack on papers discussing Direct3D 12 and Vulkan, and should therefore be considered a field that is worthwhile looking into.
